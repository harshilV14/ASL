
# ü§ü ASL to Text

**Communication** is the act of transferring information from one place to another. Being able to communicate effectively is perhaps the most important of all life skills. It is what enables us to pass information to other people, and to understand what is said to us.
Sign languages are an extremely important communication tool for many deaf and hard-of-hearing people and also for people who cannot speak.

The **ASL to Text Converter** is a project aimed at translating American Sign Language (ASL) hand gestures into written text. This tool leverages machine learning and computer vision techniques to interpret sign language and provide accurate translations, making communication more accessible for deaf and hard-of-hearing individuals.

## üöÄ Features

- **Real-time Gesture Recognition**: Translates ASL signs into text instantly.
- **High Accuracy**: Built on a robust machine learning model trained on a diverse dataset.
- **Intuitive Interface**: User-friendly design for seamless interaction.
- **Customizable Vocabulary**: Users can add or modify signs for personal use.

## üõ† Built With

We have mainly used Python and some of its libraries and tools to build this project. Some of these are:

- **üü° cvzone**: Used to access the camera to capture hand gestures, including the built-in `HandDetector` library for hand detection.
  
- **üî¢ Numpy**: Used for model predictions, specifically for predicting different hand signs and the letters used.

- **üñê Mediapipe**: A cross-platform framework developed by Google for processing video and multimedia. It provides advanced capabilities for hand tracking and gesture recognition, making it ideal for real-time ASL recognition.

- **üîç OpenCV**: An open-source computer vision and machine learning library that provides tools for image and video processing, essential for capturing and manipulating frames from a webcam.

- **üß† TensorFlow**: An open-source machine learning framework developed by Google, widely used for building and training machine learning models. In this project, TensorFlow runs the neural network model that recognizes ASL gestures.

- **ü§ñ Teachable Machine**: A user-friendly web tool that allows anyone to create machine learning models without coding, designed for tasks like image classification and pose detection.

## üì¶ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/ChirayuDodiya/ASL-to-Text

2. Install required packages:
   ```bash
    pip install -r requirements.txt

## üèó Process Overview

The process of developing the ASL to Text Converter involves three main steps: data collection, model training, and testing. Here's how each step works:

### 1. Data Collection

- **File**: `collect.py`
- **Description**: This script is used to collect data for training the model. 
- **How It Works**:
  1. Run the `collect.py` script.
  2. The script opens a user interface where you can click on letters to indicate the corresponding ASL sign. The Data is stored in \Data folder.
  3. Each click records the hand gesture as a data sample, which is saved for training.

### 2. Model Training

- **Tool**: Teachable Machine by Google
- **Description**: The collected data is uploaded to Teachable Machine, where it is used to train the model.
- **How It Works**:
  1. Once you have collected sufficient data, export it from `collect.py`.
  2. Upload the dataset to Teachable Machine.
  3. Train the model using the uploaded data to recognize the ASL signs.
  4. After training, export the model as a Keras model file (usually in `.h5` format).

### 3. Testing

- **File**: `test.py`
- **Description**: This script is used to test the trained model.
- **How It Works**:
  1. Run the `test.py` script.
  2. The script loads the trained Keras model.
  3. It accesses the camera to capture live hand gestures.
  4. The model predicts the corresponding ASL sign in real-time, displaying the recognized letter on the screen.



## ü§ù Contributors

- [@Chirayu Dodiya](https://github.com/ChirayuDodiya)
- [@Mayank](https://github.com/likemacc)
- [@Nandini Gadhvi](https://github.com/NadiniGadhvi)
- [@Abhishek Guna](https://github.com/HINOKAM-ii)
- [@Harshil Vasava](https://github.com/harshilV14)


## üé• Demo

In this section, we provide a demonstration of the ASL to Text Converter. Below are the steps to showcase how the application works, along with visual aids.

### How to Use the ASL to Text Converter

1. **Start the Application**:
   - Run the `test.py` script to initiate the application.
   - Ensure your webcam is connected and accessible.

2. **User Interface**:
   - Upon running, a window will open showing the live feed from your webcam.

3. **Perform ASL Signs**:
   - Position your hand in front of the camera and perform the ASL signs.
   - The model will recognize the sign in real-time and display the corresponding letter on the screen.

### Example Demonstration

#### Sample Signs:

| ASL Sign | Description                     |
|----------|---------------------------------|
| ![A](link_to_A_sign_image) | ASL Sign for Letter A             |
| ![B](link_to_B_sign_image) | ASL Sign for Letter B             |
| ![C](link_to_C_sign_image) | ASL Sign for Letter C             |

#### Video Demonstration

Watch the following video to see the ASL to Text Converter in action:

[![ASL to Text Demo](link_to_video_thumbnail)](link_to_full_video)

### Notes

- Ensure you are in a well-lit area for better gesture recognition.
- If the model does not recognize your signs accurately, consider retraining the model with more diverse data samples.

### Feedback

We welcome feedback! If you have any suggestions or issues, please open an issue on our GitHub repository.



